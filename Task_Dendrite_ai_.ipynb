{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reading the json file and data i.e., iris.csv file"
      ],
      "metadata": {
        "id": "MrpwA11I9jUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary libraries\n",
        "import pandas as pd\n",
        "import json"
      ],
      "metadata": {
        "id": "75fbNJfdBDJH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reading the given json file into the notebook.\n",
        "with open('algoparams_from_ui.json', 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "data"
      ],
      "metadata": {
        "id": "Jo4jbxYdBLwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the dataset into the notebook.\n",
        "iris = pd.read_csv('/content/iris.csv')"
      ],
      "metadata": {
        "id": "nb5rA25R9ct8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Read the target and type of regression to be run"
      ],
      "metadata": {
        "id": "-m5-7SKW9d2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# slecting the target feature as mentioned in the json file.\n",
        "target = data['design_state_data']['target']['target']"
      ],
      "metadata": {
        "id": "AuJClGGSAd6m"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the type of operation\n",
        "ty = data['design_state_data']['target']['type']\n",
        "ty"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JR0ck7c-BxQB",
        "outputId": "c0f311a6-48be-43b1-e23d-45d17b577a2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'regression'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Since the type is reggression we can ignore the species feature. It is not a continous feature.(it is a discrete categorical feature)\n",
        "if ty == 'regression':\n",
        "  iris = iris.drop('species', axis=1)"
      ],
      "metadata": {
        "id": "vbyj6TemPwyx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the data as X - independent features and y - dependent feature\n",
        "X = iris.drop(target,axis=1 ).values\n",
        "y = iris[target].values"
      ],
      "metadata": {
        "id": "Jqf1yixvLk7L"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing X and y to check they where splitted correctly\n",
        "X, y"
      ],
      "metadata": {
        "id": "fi2celPnQBtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Read the features (which are column names in the csv) and figure out what missing imputation needs to be applied and apply that to the columns loaded in a dataframe"
      ],
      "metadata": {
        "id": "UpNR7tRvAeS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# When the given type is classification we need to encode the species feature otherwise not needed.\n",
        "if ty == 'classification':\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "  le = LabelEncoder()\n",
        "  iris['species'] = le.fit_transform(iris[['species']])"
      ],
      "metadata": {
        "id": "HHnxGboMItEj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing Imutation of missing values and feature scaling.\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "# Feature Handling\n",
        "preprocessed_features = {}\n",
        "for feature_name, feature_data in data['design_state_data']['feature_handling'].items():\n",
        "    feature_details = feature_data['feature_details']\n",
        "    if 'missing_values' in feature_details:\n",
        "        missing_values = feature_details['missing_values']\n",
        "        impute_method = missing_values if isinstance(missing_values, str) else missing_values['impute_with']\n",
        "        if impute_method == 'Impute':\n",
        "            if isinstance(missing_values, str):\n",
        "                impute_value = 0\n",
        "            else:\n",
        "                impute_value = missing_values['impute_value']\n",
        "            imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean', fill_value = impute_value)\n",
        "            for column in iris.columns:\n",
        "              iris[column] = imputer.fit_transform(iris[[column]])\n",
        "    else:\n",
        "        impute_method = None\n",
        "    if 'rescaling' in feature_details:\n",
        "        rescaling_method = feature_details['rescaling']\n",
        "        if rescaling_method == 'Standardization':\n",
        "            scaler = StandardScaler()\n",
        "            for column in target.columns:\n",
        "              target[column] = scaler.fit_transform(target[[column]])\n",
        "    else:\n",
        "        rescaling_method = None"
      ],
      "metadata": {
        "id": "5d5HcOeqAixY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Compute feature reduction based on input. See the screenshot below where there can be No Reduction, Corr with Target, Tree-based, PCA. Please make sure you write code so that all options can work. If we rerun your code with a different Json it should work if we switch No Reduction to say PCA.\n"
      ],
      "metadata": {
        "id": "NJvyMw4IAjJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pca\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Extract options from input\n",
        "reduction_option = data['design_state_data']['feature_reduction']['feature_reduction_method']\n",
        "n_features_to_keep = data['design_state_data']['feature_reduction']['num_of_features_to_keep']\n",
        "n_components = 2\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Defining a function to perform the specified reduction technique\n",
        "def perform_feature_reduction(X_train, X_test, y_train, reduction_option, n_features_to_keep=None, n_components=None):\n",
        "    if reduction_option == \"No Reduction\":\n",
        "        return X_train, X_test\n",
        "    elif reduction_option == \"Corr with Target\":\n",
        "        # Selecting top features based on correlation with target\n",
        "        selector = SelectKBest(score_func=f_classif, k=n_features_to_keep)\n",
        "        selector.fit(X_train, y_train)\n",
        "        X_train = selector.transform(X_train)\n",
        "        X_test = selector.transform(X_test)\n",
        "        return X_train, X_test\n",
        "    elif reduction_option == \"Tree-based\":\n",
        "        # Selecting top features based on feature importances from a tree-based model\n",
        "        model = RandomForestRegressor(random_state=42)\n",
        "        model.fit(X_train, y_train)\n",
        "        feature_importances = model.feature_importances_\n",
        "        top_features_indices = np.argsort(feature_importances)[::-1][:int(n_features_to_keep)]\n",
        "        print(\"X_train shape before slicing:\", X_train.shape)\n",
        "        print(\"X_test shape before slicing:\", X_test.shape)\n",
        "        print(\"top_features_indices:\", top_features_indices)\n",
        "        X_train = X_train[:, top_features_indices]\n",
        "        X_test = X_test[:, top_features_indices]\n",
        "        print(\"X_train shape after slicing:\", X_train.shape)\n",
        "        print(\"X_test shape after slicing:\", X_test.shape)\n",
        "        return X_train, X_test\n",
        "    elif reduction_option == \"PCA\":\n",
        "        # Performing PCA to reduce dimensions\n",
        "        pca = PCA(n_components=n_components)\n",
        "        X_train = pca.fit_transform(X_train)\n",
        "        X_test = pca.transform(X_test)\n",
        "        return X_train, X_test\n",
        "# Performing the specified feature reduction\n",
        "X_train_reduced, X_test_reduced = perform_feature_reduction(X_train, X_test, y_train, reduction_option, n_features_to_keep, n_components)\n",
        "# Now we can use X_train_reduced and X_test_reduced for further modeling and analysis\n"
      ],
      "metadata": {
        "id": "N89xnGR2Amtn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb273d18-7cf6-48e9-fada-368d6fb02da9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape before slicing: (120, 3)\n",
            "X_test shape before slicing: (30, 3)\n",
            "top_features_indices: [2 1 0]\n",
            "X_train shape after slicing: (120, 3)\n",
            "X_test shape after slicing: (30, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Parse the Json and make the model objects (using sklean) that can handle what is required in the “prediction_type” specified in the JSON (See #1 where “prediction_type” is specified). Keep in mind not to pick models that don’t apply for the prediction_type specified\n",
        "\n",
        "\n",
        "5) Run the fit and predict on each model – keep in mind that you need to do hyper parameter tuning i.e., use GridSearchCV"
      ],
      "metadata": {
        "id": "gBAXwoDgAnB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, ExtraTreesRegressor,ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
        "\n",
        "# Iterating through the algorithms and their parameters\n",
        "for algo_name, algo_params in data['design_state_data']['algorithms'].items():\n",
        "    if algo_params['is_selected']:\n",
        "        print(f\"Training {algo_name}...\")\n",
        "        if algo_name == \"RandomForestClassifier\":\n",
        "          model = RandomForestClassifier()\n",
        "          param_grid = {\n",
        "        'n_estimators': range(algo_params['min_trees'], algo_params['max_trees'] + 1),\n",
        "        'max_depth': range(algo_params['min_depth'], algo_params['max_depth'] + 1),\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': range(algo_params['min_samples_per_leaf_min_value'], algo_params['min_samples_per_leaf_max_value'] + 1),\n",
        "        #'max_features': ['auto', 1.0],\n",
        "\n",
        "    }\n",
        "        elif algo_name == \"RandomForestRegressor\":\n",
        "            model = RandomForestRegressor()\n",
        "            param_grid = {\n",
        "        'n_estimators': range(algo_params['min_trees'], algo_params['max_trees'] + 1),\n",
        "        'max_depth': range(algo_params['min_depth'], algo_params['max_depth'] + 1),\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': range(algo_params['min_samples_per_leaf_min_value'], algo_params['min_samples_per_leaf_max_value'] + 1),\n",
        "        #'max_features': ['auto', 1.0],\n",
        "\n",
        "    }\n",
        "        elif algo_name == \"GBTClassifier\":\n",
        "            model = GradientBoostingClassifier()\n",
        "            param_grid = {\n",
        "        'n_estimators': algo_params['num_of_BoostingStages'],\n",
        "        'learning_rate': algo_params['learningRate'],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'max_depth': range(algo_params['min_depth'], algo_params['max_depth'] + 1),\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "\n",
        "    }\n",
        "        elif algo_name == \"GBTRegressor\":\n",
        "            model = GradientBoostingRegressor()\n",
        "            param_grid = {\n",
        "        'n_estimators': algo_params['num_of_BoostingStages'],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'max_depth': range(algo_params['min_depth'], algo_params['max_depth'] + 1),\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "\n",
        "    }\n",
        "        elif algo_name == \"LinearRegression\":\n",
        "            model = LinearRegression()\n",
        "            param_grid = {\n",
        "        'fit_intercept': [True, False],\n",
        "        'normalize': [True, False],\n",
        "\n",
        "    }\n",
        "        elif algo_name == \"LogisticRegression\":\n",
        "          model = LogisticRegression()\n",
        "          param_grid = {\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'C': [0.001, 0.01, 0.1, 1, 10],\n",
        "        'fit_intercept': [True, False],\n",
        "        'solver': ['liblinear'],\n",
        "\n",
        "    }\n",
        "        elif algo_name == \"RidgeRegression\":\n",
        "            model = Ridge()\n",
        "            param_grid = {\n",
        "        'alpha': np.linspace(algo_params['min_regparam'], algo_params['max_regparam'], num=5),\n",
        "        'fit_intercept': [True, False],\n",
        "        'normalize': [True, False],\n",
        "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
        "\n",
        "    }\n",
        "        elif algo_name == \"LassoRegression\":\n",
        "            model = Lasso()\n",
        "            param_grid = {\n",
        "        'alpha': np.linspace(algo_params['min_regparam'], algo_params['max_regparam'], num=5),\n",
        "        'fit_intercept': [True, False],\n",
        "        'normalize': [True, False],\n",
        "\n",
        "    }\n",
        "        elif algo_name == \"ElasticNetRegression\":\n",
        "            model = ElasticNet()\n",
        "            param_grid = {\n",
        "        'alpha': np.linspace(algo_params['min_regparam'], algo_params['max_regparam'], num=5),\n",
        "        'l1_ratio': np.linspace(algo_params['min_elasticnet'], algo_params['max_elasticnet'], num=5),\n",
        "        'fit_intercept': [True, False],\n",
        "        'normalize': [True, False],\n",
        "\n",
        "    }\n",
        "        elif algo_name == \"xg_boost\":\n",
        "            model = XGBClassifier() if algo_params['use_gradient_boosted_tree'] else XGBRegressor()\n",
        "            param_grid = {\n",
        "        'n_estimators': algo_params['max_num_of_trees'],\n",
        "        'learning_rate': algo_params['learningRate'],\n",
        "        'max_depth': algo_params['max_depth_of_tree'],\n",
        "        'gamma': algo_params['gamma'],\n",
        "        'min_child_weight': algo_params['min_child_weight'],\n",
        "        'subsample': algo_params['sub_sample'],\n",
        "        'colsample_bytree': algo_params['col_sample_by_tree'],\n",
        "\n",
        "    }\n",
        "        elif algo_name == \"DecisionTreeRegressor\":\n",
        "            model = DecisionTreeRegressor()\n",
        "            param_grid = {\n",
        "        'max_depth': range(algo_params['min_depth'], algo_params['max_depth'] + 1),\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': algo_params['min_samples_per_leaf'],\n",
        "        'criterion': ['mse', 'friedman_mse', 'mae'],\n",
        "\n",
        "    }\n",
        "        elif algo_name == \"DecisionTreeClassifier\":\n",
        "            model = DecisionTreeClassifier()\n",
        "            param_grid = {\n",
        "        'max_depth': range(algo_params['min_depth'], algo_params['max_depth'] + 1),\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': algo_params['min_samples_per_leaf'],\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "\n",
        "    }\n",
        "        elif algo_name == \"SVM\":\n",
        "            model = SVC() if algo_params['linear_kernel'] else SVR()\n",
        "            param_grid = {\n",
        "        'C': algo_params['c_value'],\n",
        "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "        'degree': [2, 3, 4],\n",
        "        'coef0': [0.0, 1.0, 2.0],\n",
        "        'gamma': ['scale', 'auto'] if algo_params['auto'] else algo_params['custom_gamma_values'],\n",
        "\n",
        "    }\n",
        "        elif algo_name == \"SGD\":\n",
        "            model = SGDClassifier() if algo_params['use_logistics'] else SGDRegressor()\n",
        "            param_grid = {\n",
        "        'loss': ['hinge', 'log', 'modified_huber'],\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'alpha': algo_params['alpha_value'],\n",
        "        'l1_ratio': [0.15, 0.25, 0.35, 0.45, 0.55],\n",
        "        'fit_intercept': [True, False],\n",
        "        'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
        "\n",
        "    }\n",
        "        elif algo_name == \"KNN\":\n",
        "            model = KNeighborsClassifier() if algo_params['k_value'] else KNeighborsRegressor()\n",
        "            param_grid = {\n",
        "        'n_neighbors': algo_params['k_value'],\n",
        "        'weights': ['uniform', 'distance'] if algo_params['distance_weighting'] else ['uniform'],\n",
        "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "        'p': [1, 2, 3],\n",
        "\n",
        "    }\n",
        "        elif algo_name == \"extra_random_trees\":\n",
        "            model = ExtraTreesClassifier() if algo_params['is_selected'] else ExtraTreesRegressor()\n",
        "            param_grid = {\n",
        "        'n_estimators': algo_params['num_of_trees'],\n",
        "        'max_depth': algo_params['max_depth'],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': algo_params['min_samples_per_leaf'],\n",
        "        #'max_features': ['auto', 1.0],\n",
        "\n",
        "    }\n",
        "        elif algo_name == \"neural_network\":\n",
        "            model = MLPClassifier() if algo_params['is_selected'] else MLPRegressor()\n",
        "            param_grid = {\n",
        "        'hidden_layer_sizes': algo_params['hidden_layer_sizes'],\n",
        "        'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
        "        'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
        "        'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
        "        'solver': ['lbfgs', 'sgd', 'adam'],\n",
        "        'early_stopping': [True, False],\n",
        "\n",
        "    }\n",
        "\n",
        "        # Creating GridSearchCV for parameter tuning\n",
        "        grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "        grid_search.fit(X_train_reduced, y_train)\n",
        "\n",
        "        # Get the best model and evaluate on the test set\n",
        "        best_model = grid_search.best_estimator_\n",
        "        y_pred = best_model.predict(X_test_reduced)\n",
        "\n",
        "        # Evaluate the model based on the problem type (classification or regression)\n",
        "        if algo_name in [\"RandomForestClassifier\", \"GBTClassifier\", \"DecisionTreeClassifier\", \"LogisticRegression\", \"SVM\", \"SGD\", \"KNN\", \"extra_random_trees\"]:\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            print(f\"{algo_name} - Accuracy: {accuracy:.2f}\")\n",
        "        elif algo_name in [\"RandomForestRegressor\", \"GBTRegressor\", \"DecisionTreeRegressor\", \"LinearRegression\", \"RidgeRegression\", \"LassoRegression\", \"ElasticNetRegression\", \"xg_boost\", \"neural_network\"]:\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            print(f\"{algo_name} - Mean Squared Error: {mse:.2f}\")\n",
        "            print(f\"{algo_name} - r2 score: {r2_score(y_test, y_pred)}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MZCwU_WFlo9",
        "outputId": "c0ea086e-4c5b-4fc8-edf6-ffa4f0817cb3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RandomForestRegressor...\n",
            "RandomForestRegressor - Mean Squared Error: 0.04\n",
            "RandomForestRegressor - r2 score: 0.9423272002624384\n"
          ]
        }
      ]
    }
  ]
}